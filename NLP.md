# 1.装饰器用法

```python
import  time

def test_time(func):
    def calculate(*args):
        start = time.time()
        a =func(*args)
        end = time.time()
        print("cost time:",int(end-start),"s")
        return a
    return calculate
@test_time
def hhhh(*args):
    sum = 0
    for x in args:
       sum+= x
    time.sleep(2)
    return  sum
a =(x for x in range(100))
A =hhhh(*a)
print(A)
from time import sleep
from tqdm import tqdm
# for i in tqdm(range(1, 20)):
#    sleep(0.01)
#    sleep(0.5)
@test_time
def  time_tqdm(*args):
    for i in tqdm(range(1, 9)):
        sleep(0.01)
        sleep(0.5)
    return True
time_tqdm()
```

# 2.list的reverse

```python
a  = [1,2,3]
a.reverse()
print(a)#[3,2,1]
#反转：
string  = "my name is zhangcong"
strings   = string.split(" ")
strings.reverse()
```

# 3.知识图谱

## 3.1 知识图谱定义

​    知识图谱（Knowledge Graph）最先由谷歌公司提出，其开发了基于知识图谱的项目，其将知识图谱应用在语义搜索方面，通过构建起来的知识图谱可以精准的搜索出需要的信息。谷歌给予的定义为：知识图谱是谷歌用于增强其搜索引擎功能的辅助知识库，总的来讲，知识图谱就是以结构化的信息通过图结构进行关联起来的一个知识库，而基于深度学习的知识图谱的构建是将某一领域的数据信息通过深度学习算法构建“实体——关系——实体”的三元组模型，并将其存储在图结构数据库中。知识图谱（Knowledge Graph）最先由谷歌公司提出，其开发了基于知识图谱的项目，其将知识图谱应用在语义搜索方面，通过构建起来的知识图谱可以精准的搜索出需要的信息。谷歌给予的定义为：知识图谱是谷歌用于增强其搜索引擎功能的辅助知识库，总的来讲，知识图谱就是以结构化的信息通过图结构进行关联起来的一个知识库，而基于深度学习的知识图谱的构建是将某一领域的数据信息通过深度学习算法构建“实体——关系——实体”的三元组模型，并将其存储在图结构数据库中。

​    知识图谱的概念由Google公司在2012年正式提出，目的是提高搜索引擎的性能，提供更加友好的搜索结果。随后知识图谱在学术界受到了极大的关注，其构建技术也在飞速发展。目前，知识图谱已经被广泛地应用到知识问答、智能搜索、个性化推荐、软件复用、政府治理等多个领域。随着技术的不断发展，现有研究内容已经从知识图谱的实体识别、关系抽取技术扩展到了事件图谱的构建与推理技术。事件图谱刻画了现实世界中发生的事件，对事件信息进行了准确描述。事件图谱中蕴含众多事件知识，事件知识的特点是拥有众多维度，例如时间维度、逻辑维度、关系维度等。

​    事件是文本中包含的一种信息，其定义为在某个特定的时间以及特定的地点，由多个相关角色参与的一件事情或者一组事情。随着知识图谱技术的发展，越来越多的研究者开始关注一种特殊的基于事件的知识图谱，即事件图谱。在此基础上，本文将事件图谱定义为：一种以事件为中心，用来描述事件信息以及事件之间各种关系的图谱。事件图谱和知识图谱主要的不同点是事件图谱主要的研究对象是事件，描述了与事件相关的知识、事件的演变过程以及事件间的关联关系。而知识图谱主要的研究对象是实体，主要描述的是实体属性以及实体之间的关系。在事件图谱中，图的节点表示事件，图上的连边表示事件之间的时序、因果、顺承、包含等关系。

##  3.2知识图谱步骤

1. **数据采集（Data Acquisition ）** ：采集数据集一般可以通过网络爬虫、数据库获取、人工制作数据或者在相应官网上下载处理过的数据，采集的数据一般由三种形态：

- 结构化数据（Structed Data）：对于网络数据库现有的信息，可以直接进行数据库读写，这一类数据属于事先被筛选或整理成二维形式内容，因为其属于人工筛选，其置信度往往很高，因此这一类数据是作为知识图谱构建前期最主要的方式。但是由于结构化数据需要进行大量的人工操作，所以基于大量数据的情况下，以人工制作结构化数据需要的成本太高；
- 半结构化数据（Semi-Structed Data）：半结构化数据是指以web形式显示的内容，例如百度百科、维基百科等，这一类数据往往是以XML、JSON等形式存在，介于结构化与非结构化之间。这一类数据需要进行一系列的数据预处理工作，将其转换为结构化数据；
- 非结构化数据（Unstructed Data）：非结构化数据往往是没有任何结构的数据，例如图片、音频、文本等信息，这一类数据往往整体存储或读写。知识图谱的构建绝大多数需要对这些非结构化数据进行挖掘，因此知识图谱的构建主要数据来源为非结构化数据，同时相关的研究也主要以非结构化数据为“原材料”。

2. **知识提取**

* 命名实体识别（NER）：命名实体识别是对半结构化数据和非机构化数据进行信息抽取的第一步，往往实体是信息的主要载体。实体可以是人、地名等事物，也可以是某个概念。在早期通过字符串匹配或人工操作等方式将需要的实体提取出，随后人们通过自然语言处理和机器学习方式进行实体提取，而基于深度学习的知识图谱架构中，命名实体识别通过序列标注方法进行识别

* 实体关系抽取（RC）：实体关系抽取又称关系分类，为了确定“实体——关系——实体”三元组，需要对实体之间的关系进行分类，这一过程也成为语义信息的提取。早起的关系抽取采用人工方式，根据语言的语法规则进行模式匹配，这一方式虽然精度很高，但是需要各个领域的专业人士进行操作，同时需要大量的劳动力成本。基于深度学习的知识图谱架构中，通过特征工程对含有具有关系的两个实体的句子进行关系标注，实现监督学习。现如今也有基于自监督学习方式进行关系抽取。另外，Zheng等人提出的联合NER和RC的学习，将两个步骤融合一起形成联合学习方式，在一定程度上提高了模型的精准度，因此

* 属性抽取：构建起三元组后，需要对实体和关系进行属性的抽取，属性抽取往往可以直接通过网络获取，同时也可以将属性视为实体或关系，通过NER或者RC方式进行处理。

3. **知识融合**

* 实体消歧：同一个实体可能有不同种名称，同一个名称可能表示不同类型实体。例如“华东师大”和“华东师范大学”都是同一个事物，而在知识抽取过程中，并没有将其合并，因此实体消歧的主要目的是消除同名实体产生的歧义问题。参考文献[1]提供的四种方法：空间向量模型、语义模型、社会网络模型和百科知识模型可以实现实体消歧。
* 共指消解：在一个句子中，往往有多种指称项指向同一个实体，这一类问题可以通过句法分析方式进行处理，也可以通过基于机器学习算法方式转化为分类或聚类问题。
* 知识合并：往往自主建立的知识体系相对孤立，信息量有限。为了使自主构建的知识体系可以与网络现有的知识库相呼应，需要对知识进行合并，可以将以构建的知识体系以图结构存储在图形数据库中，通过实体消歧进行合并，也可以将知识体系以关系型存储在关系数据库中，并通过数据库技术进行合并。知识合并是扩大自主学习构建知识库的重要步骤。

4. **知识加工**

    通过信息抽取，可以从原始语料中提取出实体、关系与属性等知识要素．再经过知识融合，可以消除实体指称项与实体对象之间的歧义，得到一系列基本的事实表达．然而，事实本身并不等于知识，要想最终获得结构化、网络化的知识体系，还需要经历知识加工的过程．知识加工主要包括3方面内容：本体构建、知识推理和质量评估。  

   * 本体构建：本体是用于描述一个领域的术语集合（如下图），本体的目标是获取、描述和表示相关领域的知识，提供对该领域知识的共同理解，确定领域内共同认可的词汇，并从不同层次的形式化模式上给出了这些词汇(术语)和词汇间相互关系的明确定义。

   * 知识推理：顾名思义，是对知识之间的关系推理，知识推理包括逻辑关系推理和图关系推理。逻辑关系推理属于语义分析部分。例如命题“985高校一定是211，而211高校不一定是985”，由此可以推理出华东师范大学是985也是211。图关系推理根据图模型进行关系拓展，例如建立的三元组有“华东师范大学在普陀区”，“普陀区在上海市”，可以推理出“华东师范大学在上海市”。

   * 知识更新：知识是不断的更新迭代的，构建好的知识图谱需要不断的进行更新。更新方式一般有两种：全面更新和增量更新。

5. **知识存储**

## 3.1 技术

基于深度学习的知识图谱构建，主要应用深度学习框架，技术主要包括：
（1）数据采集：基于Python网络爬虫的数据采集；
（2）词向量训练：word-embedding训练，包括CBOW、Skip-gram模型以及哈夫曼树和负采样加速方法；
（3）命名实体识别：RNN，BiRNN，LSTM，BiLSTM，CRF；
（4）实体关系抽取：基于CNN的关系分类，基于依存关系模型的关系抽取；
（5）联合实体与关系抽取：复合神经网络模型Bi-LSTM+CRF+CNN，端到端（End-to-end）模型，注意力（Attention）机制等；
（6）深度学习框架：Tensorflow；
（7）数据标注：特征工程；
（8）图数据库：较为流行的图数据库有 Neo4j，Titan，OrientDB和 ArangoDB，本人常用的是Neo4j；
（9）涉及到数学知识：微积分、矩阵论（线性代数）、概率论与数理统计、最优化方法、泛函分析、数值优化等。
  Ps:现如今知识图谱的构建在科研领域是一个庞大的课程研究体系，涉及诸多技术，本人在学习过程中将不断更新和增加相关技术以适应知识图谱的发展。

## 3 事件抽取

事件是文本中包含的一种特殊信息，事件抽取就是从非结构化的文本数据中抽取与事件有关的各种角色，将信息用结构化数据表示。按照确定事件类别的方法，事件抽取可以被分为限定域事件抽取和开放域事件抽取。

### **3.1 限定域事件抽取**

限定域事件抽取是指在进行抽取任务之前，已经确定好了相应的目标事件类型和相应的结构。另外，限定域事件抽取任务还会给出一些标注数据。因为事件标注较为复杂，需要耗费一定的人力物力，所以数据集规模一般较小。在事件抽取领域中较为常用的标准数据集是ACE 2005语料库。关于限定域事件抽取的研究较多，目前研究主要采用的是深度学习方法，几种代表性的神经网络方法如下。

（1）基于注意力机制的方法

在进行事件抽取时，许多研究者使用了注意力机制，以此来提升神经网络模型的效果。注意力机制是一种仿生技术，借鉴了人类的选择性注意行为。选择性注意行为是指人类在进行观察时，视觉会快速扫描全局图像，从而确定要重点关注的内容，抑制或忽略其他无关的信息。研究者受此启发，提出了深度学习中的注意力机制，核心目标就是在众多信息中选择对于当前任务来说最关键的信息。

ACE 2005语料库给出了每个事件的事件触发词和事件论元。但是之前的研究者并没有充分利用数据集中的标注信息，更多地依赖句子的语义信息，忽略了被标注的论元信息。因此，Liu S等人提出了一种新的方法，利用论元信息来加强对触发词的识别和分类。该方法将句子中的单词信息、上下文的单词信息、上下文的实体信息结合起来，组成单词的触发词候选项。他们还采用了一种有监督的注意力机制更加深入地提取句子中的有效信息，句子中的事件触发词会比其他上下文单词获得更多的注意。最后该方法使用了一个多分类的神经网络模型完成事件抽取任务，充分利用了语料库中被标注的论元信息。

上述方法将句子中的多个事件视为独立事件，只是利用单个句子内部的信息来检测事件。但是句子中表达的事件是相互关联的，单纯地利用句子内部的信息不能很好地区分某些事件。因此Chen Y等人提出了一种分层的基于门控注意力机制的偏差标记网络，目的是融合句子和文档的信息，从而进行多事件识别和抽取。该方法采用了一种新的思路，将事件抽取看作一种序列标注问题。模型中首次添加了一个层次化的基于循环神经网络（recurrent neural network，RNN）的标签层来捕捉所有事件的依赖关系，同时设计了一种偏差目标函数来增强触发标签对模型的影响。除此之外，为了充分利用事件候选项的上下文信息，该方法采用了一种基于门控的多层次注意力机制，可以自动提取句子和文档中的信息，并进行动态的集成。该方法充分利用了事件之间的关联关系，融合了文档信息来增强事件识别的结果。可以看到，基于注意力机制的方法实现了对信息的有效提取，使得事件识别更加准确。

（2）基于预训练模型的方法

一般来说，为了更好地训练神经网络模型，需要为模型提供大规模的标注数据。但是构建大规模的标注数据耗时耗力，难以满足要求。相比之下，大规模未标注的语料却很容易构建。为了利用大量的未标注数据，研究人员提出了预训练模型。预训练模型可以从大规模的语料中提取隐含的语义信息，学习到更好的通用语义表示向量，从而提高下游任务的表现。

Yang S等人针对现有远程监督事件抽取方法中存在的问题，将目光转向预训练的语言模型，希望利用从大规模语料库中学习到的知识表示向量来提高模型的性能。其设计了一种基于预训练语言模型的事件抽取（pre-trained language model based event extractor，PLMEE）模型。该模型的结构如图2所示。他们将事件抽取看作由两个子任务组成，两个子任务分别是触发词抽取和论元抽取，并提出了以预训练语言模型为基础的触发词抽取器和论元抽取器。基于预训练模型的方法会使事件的语义表示更加精确。但是现有的方法将事件抽取看作两个子任务，构建的是流水线模型，存在明显的错误传递问题。

（3）基于图神经网络的方法

过去几年深度学习技术兴起，以神经网络为代表的技术被用来提取欧氏空间中的数据特征。但是现实世界中还有众多场景使用的是非欧氏空间数据，其中具有代表性的就是图数据。图数据被广泛应用在多种场景，如电子商务的推荐系统、知识图谱的在线推理等。但是图数据结构复杂性较高，之前的神经网络方法无法直接被使用。因此，研究人员借鉴了卷积神经网络、循环神经网络以及深度自动编码器的思想，设计了一种专门用来处理图结构数据的神经网络，即图神经网络。图卷积神经网络（graph convolutional network， GCN）是指在图数据中应用卷积操作，其核心思想是学习到一种函数对节点进行表示。通过函数变换，一个节点自身的特征可以结合其临近节点的特征，从而生成节点新的表示。



事件抽取、关系抽取等任务中，一般利用词嵌入等方法将输入序列转换为连续的向量，并没有使用句子的结构信息。为了在神经网络中引入句法结构特征，Nguyen T等人提出了一个基于句法依存树的GCN模型用于事件抽取。在GCN中，每个节点的卷积向量是由相邻节点的表示向量计算出来的，可以作为该节点的唯一特征进行分类。另外，模型中通过对当前单词的卷积向量以及句子中提到的实体进行池化操作，克服实体指称无法捕捉的问题。池化操作聚合了卷积向量，从而为事件类型预测生成了单个向量表示。该方法在事件抽取中引入了GCN模型，将句法依存树上的信息进行聚合，首次利用了句子中的结构信息。

除此之外，Liu X等人设计了一个新的联合多事件抽取（jointly multiple events extraction，JMEE）框架。该框架利用基于注意力机制的图卷积神经网络进行建模，并通过引入句法依存树中的句法捷径弧来增强信息流，以此来提升在一个句子中抽取多个事件的效果。使用句法捷径弧可以减少将信息流从一个节点转换到目标节点的转换次数。与基于序列的模型相比，该方法会使在同一个句子中从一个事件触发词跳到另一个事件触发词的跳数明显减少。GCN会利用输入的句法捷径弧，聚合目标节点一阶邻居的信息，为每个节点学习到其句法上下文的表示。之后，模型通过自注意力机制进行信息聚合，保留了多个事件之间的信息，用于抽取事件触发词和论元。基于图神经网络的方法有效地利用了句法依存树中包含的信息。但是基于图神经网络的模型的计算量比较大，且只适用于对构建好的静态图进行处理。

（4）其他神经网络方法

除了上述方法，还有众多方法被应用到事件抽取任务中，也取得了良好的效果。例如，Liu J等人设计了一种新的基于对抗模仿的知识蒸馏方法，目的是从句子中获取知识来进行事件抽取。该方法首先构建了一个教师模块，充分利用标注数据来学习知识表示，之后建立相应的学生模块用于测试。在训练过程中，鉴别器通过检测教师模块和学生模块的输出来区分两者。同时，学生模块会尽可能地模仿教师模块，生成与教师模块相似的向量来迷惑鉴别器。该方法有效地完成了知识蒸馏，得到的新模型参数量少，且性能接近复杂模型。

Hong Y等人利用具有自我调节机制的生成式对抗网络来完成事件抽取任务，提高事件抽取的性能。一般来说，生成模型产生的虚假特征往往来自语义上的伪相关上下文，在训练过程中神经网络可能会错误地、不自觉地保留记忆，从而产生虚假的特征。因此该模型采用了一种双通道自调节的学习策略来调节学习过程，还添加了一对生成判别模型。在自学习过程中，生成模型被用来生成虚假特征，而判别模型被用来消除错误。该方法减轻了虚假特征对结果的影响，提升了事件抽取的效果。

事件抽取、关系抽取等任务中，一般利用词嵌入等方法将输入序列转换为连续的向量，并没有使用句子的结构信息。为了在神经网络中引入句法结构特征，Nguyen T等人提出了一个基于句法依存树的GCN模型用于事件抽取。在GCN中，每个节点的卷积向量是由相邻节点的表示向量计算出来的，可以作为该节点的唯一特征进行分类。另外，模型中通过对当前单词的卷积向量以及句子中提到的实体进行池化操作，克服实体指称无法捕捉的问题。池化操作聚合了卷积向量，从而为事件类型预测生成了单个向量表示。该方法在事件抽取中引入了GCN模型，将句法依存树上的信息进行聚合，首次利用了句子中的结构信息。

除此之外，Liu X等人设计了一个新的联合多事件抽取（jointly multiple events extraction，JMEE）框架。该框架利用基于注意力机制的图卷积神经网络进行建模，并通过引入句法依存树中的句法捷径弧来增强信息流，以此来提升在一个句子中抽取多个事件的效果。使用句法捷径弧可以减少将信息流从一个节点转换到目标节点的转换次数。与基于序列的模型相比，该方法会使在同一个句子中从一个事件触发词跳到另一个事件触发词的跳数明显减少。GCN会利用输入的句法捷径弧，聚合目标节点一阶邻居的信息，为每个节点学习到其句法上下文的表示。之后，模型通过自注意力机制进行信息聚合，保留了多个事件之间的信息，用于抽取事件触发词和论元。基于图神经网络的方法有效地利用了句法依存树中包含的信息。但是基于图神经网络的模型的计算量比较大，且只适用于对构建好的静态图进行处理。

（4）其他神经网络方法

除了上述方法，还有众多方法被应用到事件抽取任务中，也取得了良好的效果。例如，Liu J等人设计了一种新的基于对抗模仿的知识蒸馏方法，目的是从句子中获取知识来进行事件抽取。该方法首先构建了一个教师模块，充分利用标注数据来学习知识表示，之后建立相应的学生模块用于测试。在训练过程中，鉴别器通过检测教师模块和学生模块的输出来区分两者。同时，学生模块会尽可能地模仿教师模块，生成与教师模块相似的向量来迷惑鉴别器。该方法有效地完成了知识蒸馏，得到的新模型参数量少，且性能接近复杂模型。

Hong Y等人利用具有自我调节机制的生成式对抗网络来完成事件抽取任务，提高事件抽取的性能。一般来说，生成模型产生的虚假特征往往来自语义上的伪相关上下文，在训练过程中神经网络可能会错误地、不自觉地保留记忆，从而产生虚假的特征。因此该模型采用了一种双通道自调节的学习策略来调节学习过程，还添加了一对生成判别模型。在自学习过程中，生成模型被用来生成虚假特征，而判别模型被用来消除错误。该方法减轻了虚假特征对结果的影响，提升了事件抽取的效果。

事件抽取、关系抽取等任务中，一般利用词嵌入等方法将输入序列转换为连续的向量，并没有使用句子的结构信息。为了在神经网络中引入句法结构特征，Nguyen T等人提出了一个基于句法依存树的GCN模型用于事件抽取。在GCN中，每个节点的卷积向量是由相邻节点的表示向量计算出来的，可以作为该节点的唯一特征进行分类。另外，模型中通过对当前单词的卷积向量以及句子中提到的实体进行池化操作，克服实体指称无法捕捉的问题。池化操作聚合了卷积向量，从而为事件类型预测生成了单个向量表示。该方法在事件抽取中引入了GCN模型，将句法依存树上的信息进行聚合，首次利用了句子中的结构信息。

除此之外，Liu X等人设计了一个新的联合多事件抽取（jointly multiple events extraction，JMEE）框架。该框架利用基于注意力机制的图卷积神经网络进行建模，并通过引入句法依存树中的句法捷径弧来增强信息流，以此来提升在一个句子中抽取多个事件的效果。使用句法捷径弧可以减少将信息流从一个节点转换到目标节点的转换次数。与基于序列的模型相比，该方法会使在同一个句子中从一个事件触发词跳到另一个事件触发词的跳数明显减少。GCN会利用输入的句法捷径弧，聚合目标节点一阶邻居的信息，为每个节点学习到其句法上下文的表示。之后，模型通过自注意力机制进行信息聚合，保留了多个事件之间的信息，用于抽取事件触发词和论元。基于图神经网络的方法有效地利用了句法依存树中包含的信息。但是基于图神经网络的模型的计算量比较大，且只适用于对构建好的静态图进行处理。

（4）其他神经网络方法

除了上述方法，还有众多方法被应用到事件抽取任务中，也取得了良好的效果。例如，Liu J等人设计了一种新的基于对抗模仿的知识蒸馏方法，目的是从句子中获取知识来进行事件抽取。该方法首先构建了一个教师模块，充分利用标注数据来学习知识表示，之后建立相应的学生模块用于测试。在训练过程中，鉴别器通过检测教师模块和学生模块的输出来区分两者。同时，学生模块会尽可能地模仿教师模块，生成与教师模块相似的向量来迷惑鉴别器。该方法有效地完成了知识蒸馏，得到的新模型参数量少，且性能接近复杂模型。

Hong Y等人利用具有自我调节机制的生成式对抗网络来完成事件抽取任务，提高事件抽取的性能。一般来说，生成模型产生的虚假特征往往来自语义上的伪相关上下文，在训练过程中神经网络可能会错误地、不自觉地保留记忆，从而产生虚假的特征。因此该模型采用了一种双通道自调节的学习策略来调节学习过程，还添加了一对生成判别模型。在自学习过程中，生成模型被用来生成虚假特征，而判别模型被用来消除错误。该方法减轻了虚假特征对结果的影响，提升了事件抽取的效果。

# 4.虚拟环境

> 1.Pycharm
>
> >改中文
> >
> >设置背景
> >
> >创建虚拟环境
> >
> >安装包
> >
> >设置代码模板

>
>
>2.Anaconda
>
>>
>>
>>安装Anaconda
>>
>>设置路径
>>
>>Anaconda设置下载镜像
>>
>>```
>>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --set show_channel_urls yes
>>```
>>
>>创建虚拟环境
>>
>>>1.创建环镜
>>>
>>>conda create -n tensorflow python=3.8.5
>>>
>>>2.激活环境
>>>
>>>conda activate tensorflow
>>>
>>>3.安装包
>>>
>>>conda install tensorflow
>>>
>>>pip  install tensorflow
>>>
>>>4.退出虚拟环境
>>>
>>>conda deactivate
>>>
>>>5.Pycharm进行添加虚拟环境的解释器

# 5.机器学习的算法

## 5.1 分类

**线性回归（Linear Regression）**：

- **原理**：线性回归用于建立一个线性模型，用于预测一个连续目标变量。它基于输入特征和目标变量之间的线性关系。
- **数学原理**：线性回归的目标是找到一条最适合数据的直线，使得预测值与实际值之间的误差最小化。这通常通过最小化残差平方和来实现，即最小化各个样本点的预测误差的平方之和,最小·二。

**逻辑回归（Logistic Regression）**：

- **原理**：逻辑回归用于二分类问题，预测一个样本属于某个类别的概率。它将线性回归的结果通过一个逻辑函数（sigmoid函数）映射到0和1之间的概率值。
- **数学原理**：逻辑回归使用最大似然估计来确定参数，使得给定输入特征下观测到的实际类别的概率最大化。

**决策树（Decision Trees）**：

- **原理**：决策树是一种树状结构，用于分类和回归任务。通过一系列的决策节点将数据分割成不同的类别或值。
- **数学原理**：决策树构建过程基于数据的特征，通过选择最佳的特征和阈值来划分数据。划分的目标是最大化信息增益、基尼不纯度等度量。

**随机森林（Random Forest）**：

- **原理**：随机森林是一种集成学习算法，它由多个决策树组成，通过对每个树的预测结果进行平均或投票来进行分类或回归。
- **数学原理**：随机森林的每棵树使用随机选择的样本和特征来构建，从而降低了过拟合的风险。每个树的预测结果最终汇总以产生最终的预测结果。

**支持向量机（Support Vector Machines）**：

- **原理**：支持向量机用于二分类和多分类问题，通过在特征空间中找到一个最优的超平面来分隔不同类别的数据点。
- **数学原理**：支持向量机寻找一个最大化间隔的超平面，使得数据点距离超平面的距离最大化。它可以通过拉格朗日乘数法来求解对应的优化问题。

**K最近邻算法（K-Nearest Neighbors）**：

- **原理**：K最近邻算法用于分类和回归，基于与目标样本最近的K个邻居的标签或值来进行预测。
- **数学原理**：K最近邻算法通过计算目标样本与其他样本之间的距离（如欧氏距离），选择距离最近的K个样本，然后根据这些样本的标签或值进行预测。

**朴素贝叶斯（Naive Bayes）**：

- **原理**：朴素贝叶斯是一组基于贝叶斯定理的分类算法，假设输入特征之间相互独立。
- **数学原理**：朴素贝叶斯根据训练数据中特征和类别的概率分布，计算给定特征情况下每个类别的后验概率，并选择具有最高后验概率的类别作为预测结果。

**神经网络（Neural Networks）**：

- **原理**：神经网络是受到生物神经元结构启发的模型，用于解决各种问题，从图像识别到自然语言处理。
- **数学原理**：神经网络由多个层组成，包括输入层、隐藏层和输出层。每个神经元通过加权求和和激活函数来计算输出。训练神经网络通常涉及反向传播算法，通过调整权重来最小化预测误差。

##  5.2**无监督学习算法**：

**K均值聚类（K-Means Clustering）**：

- **原理**：将数据点分成K个簇，每个簇具有相似的特征。每个簇的中心被称为质心，通过迭代优化来调整数据点和质心之间的距离。
- **数学原理**：最小化每个数据点与其所属簇的质心之间的平方距离之和，使用迭代算法（如Lloyd's算法）来实现。

**层次聚类（Hierarchical Clustering）**：

- **原理**：将数据点逐步合并成不断增大的簇，构建一个层次结构。可以通过树状图（树状图或树状图）来可视化这种层次结构。
- **数学原理**：使用不同的链接方法（如单链接、完全链接、平均链接等）来计算簇之间的距离，从而逐步合并最近的簇。

**主成分分析（PCA）**：

- **原理**：通过线性变换将高维数据投影到低维空间，以捕获最大方差的方向，以实现降维和特征提取。
- **数学原理**：计算数据的协方差矩阵，然后通过特征值分解找到主成分，这些主成分即新的特征空间轴。

**独立成分分析（Independent Component Analysis，ICA）**：

- **原理**：通过寻找数据的独立源信号，将观测信号分解为独立的非高斯分布的成分。
- **数学原理**：使用最大似然估计或最小化互信息等方法，分离混合信号的独立成分。

**高斯混合模型（Gaussian Mixture Models，GMM）**：

- **原理**：假设数据由多个高斯分布组成，通过估计这些分布的参数来对数据进行建模。
- **数学原理**：使用期望最大化（EM）算法来估计分布的均值、方差和权重，以拟合高斯混合模型。

**t-分布随机邻域嵌入（t-Distributed Stochastic Neighbor Embedding，t-SNE）**：

- **原理**：将高维数据映射到低维空间，使得在高维空间中相似的数据点在低维空间中仍然保持相似。
- **数学原理**：使用t分布来表示数据点之间的相似性，通过最小化在两个空间中相似性之间的差异来实现。

## 5.3**半监督学习算法**：

**自监督学习（Self-Supervised Learning）**：

- **原理**：利用数据本身的信息来生成辅助任务，然后使用这些任务的预测结果作为样本的标签，从而扩展监督信号。
- **数学原理**：根据不同的自监督任务（如填充缺失部分、预测旋转等），生成伪标签，然后使用这些伪标签来训练模型。

**半监督聚类（Semi-Supervised Clustering）**：

- **原理**：结合有标签的数据和无标签的数据，进行聚类分析，以帮助更好地分离簇。
- **数学原理**：使用半监督学习算法，将有标签的数据和无标签的数据结合起来进行聚类，以增强聚类性能。

## 5.4**强化学习算法**：

**Q学习（Q-Learning）**：

- **原理**：强化学习算法，用于训练代理（智能体）在与环境交互的过程中学习如何做出行动以最大化累积奖励。
- **数学原理**：通过学习一个Q值函数，代理能够选择最优行动，以最大化预期的累积奖励。

**深度Q网络（Deep Q-Networks，DQN）**：

- **原理**：结合强化学习和深度神经网络，用于解决高维、连续动作空间的问题。
- **数学原理**：使用神经网络来近似Q值函数，通过反向传播来优化网络参数，以逼近最优Q值。

**策略梯度方法（Policy Gradient Methods）**：

- **原理**：通过直接优化策略函数，学习如何选择行动以最大化预期累积奖励。
- **数学原理**：使用梯度上升法来更新策略函数，以增加对高奖励行动的选择概率。

**Actor-Critic方法**：

- **原理**：结合了策略梯度方法和值函数估计方法，用于同时学习策略和值函数，以更稳定地训练强化学习模型。

- **数学原理**：包括一个Actor网络（策略网络）和一个Critic网络（值函数估计网络），通过反馈信息来指导策略优化。

- **线性判别分析（Linear Discriminant Analysis，LDA）**：

  - **原理**：LDA用于降低数据维度，同时最大化不同类别之间的距离，最小化相同类别内部的距离，以实现有效的特征提取。
  - **数学原理**：通过计算类别均值和类别内部散度矩阵来寻找投影轴，使得在低维空间中最大化类别之间的距离。

  **集成学习算法**：

  **随机森林（Random Forest）**：

  - **原理**：随机森林由多个决策树组成，通过投票或平均来做出最终预测，以提高模型的泛化能力和稳定性。
  - **数学原理**：每个决策树使用随机选择的样本和特征来构建，从而降低过拟合风险，最终的预测结果是各个决策树的集成。

  **AdaBoost**：

  - **原理**：AdaBoost通过迭代训练多个弱分类器，加权组合它们的预测结果，以生成一个更强大的分类器。
  - **数学原理**：根据错误分类样本的权重来调整下一轮训练中的样本权重，使得弱分类器关注于之前分类错误的样本。

  **梯度提升树（Gradient Boosting Trees）**：

  - **原理**：梯度提升树通过逐步迭代地训练决策树，每一步都试图减少前一步的残差误差，从而构建一个强大的集成模型。
  - **数学原理**：使用梯度下降法来优化决策树的参数，每个新的树都试图捕获前一个树没有捕获到的模式。

  **XGBoost**：

  - **原理**：XGBoost是一种梯度提升树的扩展，通过正则化和更高效的算法来提高性能，同时支持多种损失函数。
  - **数学原理**：XGBoost通过最小化损失函数来构建每个树，同时引入了正则化项以防止过拟合。

  **LightGBM**：

  - **原理**：LightGBM是一种基于梯度提升框架的轻量级、高性能实现，使用基于直方图的算法来加速训练。
  - **数学原理**：LightGBM使用基于直方图的方式来分割数据，以降低计算复杂度，同时通过特定的分割算法来最小化损失函数。

  **CatBoost**：

  - **原理**：CatBoost是一种梯度提升树的算法，专注于处理类别特征，通过自动处理类别变量和梯度剪枝来提高性能。
  - **数学原理**：CatBoost使用特定的方法来处理类别特征，同时通过梯度剪枝来优化树的结构，以提高模型性能。

  **线性判别分析（Linear Discriminant Analysis，LDA）**：

  - **原理**：LDA用于降低数据维度，同时最大化不同类别之间的距离，最小化相同类别内部的距离，以实现有效的特征提取。
  - **数学原理**：通过计算类别均值和类别内部散度矩阵来寻找投影轴，使得在低维空间中最大化类别之间的距离。

  **集成学习算法**：

  **随机森林（Random Forest）**：

  - **原理**：随机森林由多个决策树组成，通过投票或平均来做出最终预测，以提高模型的泛化能力和稳定性。
  - **数学原理**：每个决策树使用随机选择的样本和特征来构建，从而降低过拟合风险，最终的预测结果是各个决策树的集成。

  **AdaBoost**：

  - **原理**：AdaBoost通过迭代训练多个弱分类器，加权组合它们的预测结果，以生成一个更强大的分类器。
  - **数学原理**：根据错误分类样本的权重来调整下一轮训练中的样本权重，使得弱分类器关注于之前分类错误的样本。

  **梯度提升树（Gradient Boosting Trees）**：

  - **原理**：梯度提升树通过逐步迭代地训练决策树，每一步都试图减少前一步的残差误差，从而构建一个强大的集成模型。
  - **数学原理**：使用梯度下降法来优化决策树的参数，每个新的树都试图捕获前一个树没有捕获到的模式。

  **XGBoost**：

  - **原理**：XGBoost是一种梯度提升树的扩展，通过正则化和更高效的算法来提高性能，同时支持多种损失函数。
  - **数学原理**：XGBoost通过最小化损失函数来构建每个树，同时引入了正则化项以防止过拟合。

  **LightGBM**：

  - **原理**：LightGBM是一种基于梯度提升框架的轻量级、高性能实现，使用基于直方图的算法来加速训练。
  - **数学原理**：LightGBM使用基于直方图的方式来分割数据，以降低计算复杂度，同时通过特定的分割算法来最小化损失函数。

  **CatBoost**：

  - **原理**：CatBoost是一种梯度提升树的算法，专注于处理类别特征，通过自动处理类别变量和梯度剪枝来提高性能。
  - **数学原理**：CatBoost使用特定的方法来处理类别特征，同时通过梯度剪枝来优化树的结构，以提高模型性能。

# 6.机器学习的算法

1. **相似矩阵（Similar Matrices）**：
   - 定义：两个矩阵A和B称为相似矩阵，如果存在一个可逆矩阵P，使得B = P⁻¹AP。
   - 判定方法：计算A和B的特征值，如果它们的特征值相同，则A和B是相似的。
   - 判定条件：矩阵A和B有相同的特征值。
2. **合同矩阵（Congruent Matrices）**：
   - 定义：两个矩阵A和B称为合同矩阵，如果存在一个可逆矩阵P，使得B = PᵀAP。
   - 判定方法：计算A和B的对称部分，即 (A + Aᵀ)/2 和 (B + Bᵀ)/2，然后检查它们的特征值是否相同。
   - 判定条件：矩阵A和B的对称部分有相同的特征值。
3. **正交相似性（Orthogonal Similarity）**：
   - 定义：两个实对称矩阵A和B称为正交相似的，如果存在正交矩阵P，使得B = PᵀAP。
   - 判定方法：计算A和B的特征值，如果它们的特征值相同，则可以通过正交变换使它们相似。
   - 判定条件：矩阵A和B的特征值相同。
4. **等价矩阵（Equivalent Matrices）**：
   - 定义：两个矩阵A和B称为等价矩阵，如果存在可逆矩阵P和Q，使得B = P⁻¹AQ。
   - 判定方法：计算矩阵A和B的秩，如果它们的秩相同，则可以找到等价变换使它们相等。
   - 判定条件：矩阵A和B的秩相同。
5. **正定矩阵（Positive Definite Matrix）**：
   - 定义：一个矩阵A是正定矩阵，如果对于任何非零向量x，都有xᵀAx > 0。
   - 判定方法：计算A的所有主子矩阵的行列式，如果它们都大于零，则A是正定的。
   - 判定条件：矩阵A的所有主子矩阵的行列式大于零。
6. **Hermite矩阵（Hermitian Matrix）**：
   - 定义：一个复数矩阵A是Hermite矩阵，如果满足A = Aᴴ（共轭转置）。
   - 判定方法：检查矩阵A的主对角线上的元素是否为实数，且非对角线上的元素满足a_ij = conj(a_ji)。
   - 判定条件：矩阵A的主对角线上的元素是实数，且非对角线上的元素满足a_ij = conj(a_ji)。
7. **正交矩阵（Orthogonal Matrix）**：
   - 定义：一个实矩阵A是正交矩阵，如果满足AᵀA = I。
   - 判定方法：计算矩阵A的转置乘以自身，如果结果是单位矩阵，则A是正交的。
   - 判定条件：A的转置乘以自身等于单位矩阵。
8. **对角矩阵（Diagonal Matrix）**：
   - 定义：一个矩阵A是对角矩阵，如果只有主对角线上的元素非零，其他元素均为零。
   - 判定方法：检查A的非主对角线上的元素是否都为零。
   - 判定条件：A的非主对角线上的元素都为零。
